{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8OCgP2BmcVjsSIVE6l3xm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douglaslpo/deep-learning/blob/main/RNA_Dog_predicty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importações das Bibliotecas"
      ],
      "metadata": {
        "id": "5Qln9JyuaUka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install d2l"
      ],
      "metadata": {
        "id": "3KDcLBvxr0AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7Se38SxZ2Bx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importação do Dataset"
      ],
      "metadata": {
        "id": "X7XCWKGYayES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sobre o dataset\n",
        "Os dados da competição são divididos em um conjunto de treinamento e um conjunto de teste. O conjunto de treinamento contém imagens e o conjunto de teste contém imagens. As imagens em ambos os conjuntos estão no formato JPEG. Essas imagens contêm três canais RGB (cores) e diferentes alturas e larguras. Existem 120 raças de cães no conjunto de treinamento, incluindo Labradores, Poodles, Dachshunds, Samoyeds, Huskies, Chihuahuas e Yorkshire Terriers."
      ],
      "metadata": {
        "id": "zLWV0PbEnJtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@save\n",
        "d2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',\n",
        "                            '0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')\n",
        "\n",
        "# If you use the full dataset downloaded for the Kaggle competition, change\n",
        "# the variable below to False\n",
        "demo = True\n",
        "if demo:\n",
        "    data_dir = d2l.download_extract('dog_tiny')\n",
        "else:\n",
        "    data_dir = os.path.join('..', 'data', 'dog-breed-identification')\n"
      ],
      "metadata": {
        "id": "juaGfnYpsMJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importando o dataset do drive para não perder o mesmo quando reiniciar o colab!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Caminho onde se encontra o meu DataFrame no meu Drive!\n",
        "label_dog = '/content/drive/MyDrive/datasets/labels.csv'\n",
        "\n",
        "# Carregando o Dataframe\n",
        "df = pd.read_csv(label_dog)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-79ZEFpvOGsa",
        "outputId": "3807aeed-3bf2-414d-9f67-44c9ff0cd0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Organizando o dataset\n",
        "Podemos organizar o conjunto de dados de forma semelhante ao que fizemos em Section 13.13, nomeadamente separando um conjunto de validação do conjunto de treinamento e movendo imagens em subpastas agrupadas por rótulos.\n",
        "\n",
        "A função reorg_dog_data abaixo é usada para ler os rótulos dos dados de treinamento, segmentar o conjunto de validação e organizar o conjunto de treinamento."
      ],
      "metadata": {
        "id": "4xoAo-kFnlJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reorg_dog_data(data_dir, valid_ratio):\n",
        "    labels = d2l.read_csv_labels(os.path.join(data_dir, 'labels.csv'))\n",
        "    d2l.reorg_train_valid(data_dir, labels, valid_ratio)\n",
        "    d2l.reorg_test(data_dir)\n",
        "\n",
        "\n",
        "batch_size = 4 if demo else 128\n",
        "valid_ratio = 0.1\n",
        "reorg_dog_data(data_dir, valid_ratio)\n"
      ],
      "metadata": {
        "id": "1CyZNYfYnsnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aumentando as imagens\n",
        "O tamanho das imagens nesta seção é maior do que as imagens na seção anterior. Aqui estão mais algumas operações de aumento de imagem que podem ser úteis."
      ],
      "metadata": {
        "id": "-WX-GHXZn2__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = torchvision.transforms.Compose([\n",
        "    # Randomly crop the image to obtain an image with an area of 0.08 to 1 of\n",
        "    # the original area and height to width ratio between 3/4 and 4/3. Then,\n",
        "    # scale the image to create a new image with a height and width of 224\n",
        "    # pixels each\n",
        "    torchvision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n",
        "                                             ratio=(3.0/4.0, 4.0/3.0)),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    # Randomly change the brightness, contrast, and saturation\n",
        "    torchvision.transforms.ColorJitter(brightness=0.4,\n",
        "                                       contrast=0.4,\n",
        "                                       saturation=0.4),\n",
        "    # Add random noise\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    # Standardize each channel of the image\n",
        "    torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                     [0.229, 0.224, 0.225])])\n"
      ],
      "metadata": {
        "id": "EdRV2hk5oIet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Durante o teste, usamos apenas operações de pré-processamento de imagens definidas."
      ],
      "metadata": {
        "id": "_nrI5dTUoJ2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_test = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(256),\n",
        "    # Crop a square of 224 by 224 from the center of the image\n",
        "    torchvision.transforms.CenterCrop(224),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                     [0.229, 0.224, 0.225])])\n"
      ],
      "metadata": {
        "id": "XrBwLi9woUAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leitura do dataset\n",
        "Como na seção anterior, podemos criar uma instância ImageFolderDataset para ler o conjunto de dados contendo os arquivos de imagem originais."
      ],
      "metadata": {
        "id": "WX5HVofYoj4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, 'train_valid_test', folder),\n",
        "    transform=transform_train) for folder in ['train', 'train_valid']]\n",
        "\n",
        "valid_ds, test_ds = [torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, 'train_valid_test', folder),\n",
        "    transform=transform_test) for folder in ['valid', 'test']]\n"
      ],
      "metadata": {
        "id": "hh2rNLYaoq-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Aqui, criamos instâncias de DataLoader"
      ],
      "metadata": {
        "id": "LQjnErHMo2u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, train_valid_iter = [torch.utils.data.DataLoader(\n",
        "    dataset, batch_size, shuffle=True, drop_last=True)\n",
        "    for dataset in (train_ds, train_valid_ds)]\n",
        "\n",
        "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\n",
        "                                         drop_last=True)\n",
        "\n",
        "test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\n",
        "                                        drop_last=False)\n"
      ],
      "metadata": {
        "id": "ZOjREGqWo4W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definição do Modelo\n",
        "O conjunto de dados para esta competição é um subconjunto dos dados ImageNet. Portanto, podemos usar a abordagem discutida em Section 13.2 para selecionar um modelo pré-treinado em todo o conjunto de dados ImageNet e usá-lo para extrair recursos de imagem a serem inseridos na rede de saída de pequena escala personalizada. A Gluon oferece uma ampla gama de modelos pré-treinados. Aqui, usaremos o modelo ResNet-34 pré-treinado. Como o conjunto de dados da competição é um subconjunto do conjunto de dados de pré-treinamento, simplesmente reutilizamos a entrada da camada de saída do modelo pré-treinado, ou seja, os recursos extraídos. Então, podemos substituir a camada de saída original por uma pequena camada personalizada de rede de saída que pode ser treinada, como duas camadas totalmente conectadas em uma série. Diferente da experiência em Section 13.2, aqui, não retreinamos o modelo pré-treinado usado para extração de características. Isso reduz o tempo de treinamento e a memória necessária para armazenar gradientes de parâmetro do modelo.\n",
        "\n",
        "Você deve notar que, durante o aumento da imagem, usamos os valores médios e desvios padrão dos três canais RGB para todo o conjunto de dados ImageNet para normalização. Isso é consistente com a normalização do modelo pré-treinado."
      ],
      "metadata": {
        "id": "OrXxOxIApB8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_net(devices):\n",
        "    finetune_net = nn.Sequential()\n",
        "    finetune_net.features = torchvision.models.resnet34(pretrained=True)\n",
        "    # Define a new output network\n",
        "    # There are 120 output categories\n",
        "    finetune_net.output_new = nn.Sequential(nn.Linear(1000, 256),\n",
        "                                            nn.ReLU(),\n",
        "                                            nn.Linear(256, 120))\n",
        "    # Move model to device\n",
        "    finetune_net = finetune_net.to(devices[0])\n",
        "    # Freeze feature layer params\n",
        "    for param in finetune_net.features.parameters():\n",
        "        param.requires_grad = False\n",
        "    return finetune_net\n"
      ],
      "metadata": {
        "id": "DeIqBGVhpa6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ao calcular a perda, primeiro usamos a variável-membro features para obter a entrada da camada de saída do modelo pré-treinado, ou seja, a característica extraída. Em seguida, usamos essa característica como a entrada para nossa pequena rede de saída personalizada e calculamos a saída."
      ],
      "metadata": {
        "id": "3Q8aSW6Bpd9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "def evaluate_loss(data_iter, net, devices):\n",
        "    l_sum, n = 0.0, 0\n",
        "    for features, labels in data_iter:\n",
        "        features, labels = features.to(devices[0]), labels.to(devices[0])\n",
        "        outputs = net(features)\n",
        "        l = loss(outputs, labels)\n",
        "        l_sum = l.sum()\n",
        "        n += labels.numel()\n",
        "    return l_sum / n\n"
      ],
      "metadata": {
        "id": "SRbGPoTWpfqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo as funções de treinamento\n",
        "Selecionaremos o modelo e ajustaremos os hiperparâmetros de acordo com o desempenho do modelo no conjunto de validação. A função de treinamento do modelo treinar apenas treina a pequena rede de saída personalizada."
      ],
      "metadata": {
        "id": "ZIbXtuUupn-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
        "          lr_decay):\n",
        "    # Only train the small custom output network\n",
        "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
        "    trainer = torch.optim.SGD((param for param in net.parameters()\n",
        "                               if param.requires_grad), lr=lr,\n",
        "                              momentum=0.9, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\n",
        "    num_batches, timer = len(train_iter), d2l.Timer()\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
        "                            legend=['train loss', 'valid loss'])\n",
        "    for epoch in range(num_epochs):\n",
        "        metric = d2l.Accumulator(2)\n",
        "        for i, (features, labels) in enumerate(train_iter):\n",
        "            timer.start()\n",
        "            features, labels = features.to(devices[0]), labels.to(devices[0])\n",
        "            trainer.zero_grad()\n",
        "            output = net(features)\n",
        "            l = loss(output, labels).sum()\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            metric.add(l, labels.shape[0])\n",
        "            timer.stop()\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "                animator.add(epoch + (i + 1) / num_batches,\n",
        "                             (metric[0] / metric[1], None))\n",
        "        if valid_iter is not None:\n",
        "            valid_loss = evaluate_loss(valid_iter, net, devices)\n",
        "            animator.add(epoch + 1, (None, valid_loss))\n",
        "        scheduler.step()\n",
        "    if valid_iter is not None:\n",
        "        print(f'train loss {metric[0] / metric[1]:.3f}, '\n",
        "              f'valid loss {valid_loss:.3f}')\n",
        "    else:\n",
        "        print(f'train loss {metric[0] / metric[1]:.3f}')\n",
        "    print(f'{metric[1] * num_epochs / timer.sum():.1f} examples/sec '\n",
        "          f'on {str(devices)}')\n"
      ],
      "metadata": {
        "id": "8n47P5sZptHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento e Validação do Modelo\n",
        "Agora podemos treinar e validar o modelo. Os hiperparâmetros a seguir podem ser ajustados. Por exemplo, podemos aumentar o número de épocas. Como lr_period elr_decay são definidos como 10 e 0,1 respectivamente, a taxa de aprendizado do algoritmo de otimização será multiplicada por 0,1 a cada 10 épocas."
      ],
      "metadata": {
        "id": "W4PHAp3Wp0MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "devices, num_epochs, lr, wd = d2l.try_all_gpus(), 5, 0.001, 1e-4\n",
        "lr_period, lr_decay, net = 10, 0.1, get_net(devices)\n",
        "train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
        "      lr_decay)\n"
      ],
      "metadata": {
        "id": "a0VBpdEVp97I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Classificando o Conjunto de Testes e Enviando Resultados no Kaggle\n",
        " Depois de obter um design de modelo satisfatório e hiperparâmetros, usamos todos os conjuntos de dados de treinamento (incluindo conjuntos de validação) para treinar novamente o modelo e, em seguida, classificar o conjunto de teste. Observe que as previsões são feitas pela rede de saída que acabamos de treinar."
      ],
      "metadata": {
        "id": "AXKMCkh9qHxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = get_net(devices)\n",
        "train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n",
        "      lr_decay)\n",
        "\n",
        "preds = []\n",
        "for data, label in test_iter:\n",
        "    output = torch.nn.functional.softmax(net(data.to(devices[0])), dim=0)\n",
        "    preds.extend(output.cpu().detach().numpy())\n",
        "ids = sorted(os.listdir(\n",
        "    os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))\n",
        "with open('submission.csv', 'w') as f:\n",
        "    f.write('id,' + ','.join(train_valid_ds.classes) + '\\n')\n",
        "    for i, output in zip(ids, preds):\n",
        "        f.write(i.split('.')[0] + ',' + ','.join(\n",
        "            [str(num) for num in output]) + '\\n')\n"
      ],
      "metadata": {
        "id": "xTKgdKN_qPVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resumo da obra\n",
        "Podemos usar um modelo pré-treinado no conjunto de dados ImageNet para extrair recursos e treinar apenas uma pequena rede de saída personalizada. Isso nos permitirá classificar um subconjunto do conjunto de dados ImageNet com menor sobrecarga de computação e armazenamento."
      ],
      "metadata": {
        "id": "oS0iqaqRqUFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercicios\n",
        "\n",
        "- Ao usar todo o conjunto de dados Kaggle, que tipo de resultados você obtém ao aumentar batch_size (tamanho do lote) enum_epochs (número de épocas)?\n",
        "\n",
        "- Você obtém melhores resultados se usar um modelo pré-treinado mais profundo?\n",
        "\n",
        "- Digitalize o código QR para acessar as discussões relevantes e trocar ideias sobre os métodos usados e os resultados obtidos com a comunidade. Você pode sugerir técnicas melhores?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RURzlEI1qaNP"
      }
    }
  ]
}