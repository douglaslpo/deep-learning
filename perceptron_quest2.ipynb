{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Atividade Disciplina: Deep Learning\n",
        "\n",
        "\n",
        "  * Essa atividade está dividida em duas partes. Ao término, poste a atividade no link disponibilizado no moodle\n",
        "\n",
        "## Parte 1:\n",
        "\n",
        "1.   Abra o colab.research.google.com e crie um novo notebook.\n",
        "2.   Implemente o algoritmo abaixo para atualização dos pesos da rede perceptron que considera a atualização de pesos:\n",
        "\n",
        " * Predição(w, x, bias)\n",
        "\tativação = wT * x + bias\n",
        "\tretorna 1 se ativação >=0 senão 0\n",
        " * Repita (até a quantidade de épocas)\n",
        " * Para cada instância (x,y)\n",
        "\terro=yi-predição(w, x, bias)\n",
        "\tbias=bias + α *erro\n",
        " * Para cada peso wk\n",
        "\tw[k] = w[k]+ α*erro *x[k]\n",
        "\n",
        "\n",
        ">Considere os pesos e o bias inicial iguais a zero, taxa de aprendizado = 0.1 e número de épocas=5.\n",
        "Ainda, considere o seguinte conjunto de entrada:\n",
        "2,781083600\t\t2,550537003\t\t0\n",
        "1,465489372\t\t2,362125076\t\t0\n",
        "3,396561688\t\t4,400293529\t\t0\n",
        "1,38807019\t\t1,850220317\t\t0\n",
        "3,06407232\t\t3,005305973\t\t0\n",
        "7,627531214\t\t2,759262235\t\t1\n",
        "5,332441248\t\t2,088626775\t\t1\n",
        "6,922596716\t\t1,77106367\t\t1\n",
        "8,675418651\t\t-0,242068655\t\t1\n",
        "7,673756466\t\t3,508563011\t\t1\n",
        "\n",
        "* O resultado esperado é:\n",
        "Peso= [0.20653640140000007, -0.23418117710000003]\n",
        "Bias: -0.1\n",
        "\n",
        "- *Sua implementação deve seguir o preenchimento do código descrito abaixo:*\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "def predicao(w,x,bias):\n",
        "   #SEU CODIGO AQUI\n",
        "\n",
        "\n",
        "def main():\n",
        "   x_instances=[[2.7810836, 2.550537003],[1.465489372, 2.362125076],[3.396561688, 4.400293529], [1.38807019, 1.850220317],[3.06407232, 3.005305973],[7.627531214, 2.759262235],[5.332441248, 2.088626775],[6.922596716, 1.77106367],[8.675418651, -0.242068655],[7.673756466, 3.508563011]]\n",
        "   yi_label_instances=[0,0,0,0,0,1,1,1,1,1]\n",
        "   #SEU CODIGO AQUI\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   main()\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "99DFpZ0nFS3o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTKFzqjjC8Uh",
        "outputId": "58024ba6-a184-456a-8746-6b7ae5595ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peso= [0.20653640140000007, -0.23418117710000003]\n",
            "Bias: -0.1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def predicao(w, x, bias):\n",
        "    ativação = np.dot(w, x) + bias\n",
        "    return 1 if ativação >= 0 else 0\n",
        "\n",
        "def main():\n",
        "    x_instances = [[2.7810836, 2.550537003],[1.465489372, 2.362125076],[3.396561688, 4.400293529], [1.38807019, 1.850220317],[3.06407232, 3.005305973],[7.627531214, 2.759262235],[5.332441248, 2.088626775],[6.922596716, 1.77106367],[8.675418651, -0.242068655],[7.673756466, 3.508563011]]\n",
        "    yi_label_instances = [0,0,0,0,0,1,1,1,1,1]\n",
        "\n",
        "    # Inicialização dos pesos e bias\n",
        "    w = [0, 0]\n",
        "    bias = 0\n",
        "\n",
        "    # Parâmetros\n",
        "    alpha = 0.1\n",
        "    num_epochs = 5\n",
        "\n",
        "    # Treinamento do Perceptron\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, x in enumerate(x_instances):\n",
        "            yi = yi_label_instances[i]\n",
        "            erro = yi - predicao(w, x, bias)\n",
        "            bias = bias + alpha * erro\n",
        "            for j in range(len(w)):\n",
        "                w[j] = w[j] + alpha * erro * x[j]\n",
        "\n",
        "    print(\"Peso=\", w)\n",
        "    print(\"Bias:\", bias)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2:\n",
        "\n",
        "3. Seja a seguinte alteração do algoritmo anterior:\n",
        "\n",
        "```\n",
        "Predição(w, x, bias)\n",
        "\tativação = sigmoid(wT * x + bias)\n",
        "\tretorna ativação\n",
        "\n",
        "Repita (até convergência dos pesos)\n",
        "Para cada instância (x,y)\n",
        "\tbias=bias - α *gradiente_bias\n",
        "Para cada peso wk\n",
        "\tw[k] = w[k] - α*gradiente_x[k]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Assumindo que, no algoritmo anterior, o cálculo dos gradientes para os pesos seja:\n",
        "gradente_bias=(y-ŷ)*(sigmoid(w[k]*x[k]+bias)*(1-sigmoid(w[k]*x[k]+bias)))\n",
        "gradente=(y-ŷ)*(sigmoid(w[k]*x[k]+bias)*(1-sigmoid(w[k]*x[k]+bias)))*x[k]\n",
        "\n",
        "# Crie uma segunda versão do exercício contemplando o algoritmo alterado abaixo.\n"
      ],
      "metadata": {
        "id": "YIQJXLwDGiJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def predicao(w, x, bias):\n",
        "    ativação = sigmoid(np.dot(w, x) + bias)\n",
        "    return ativação\n",
        "\n",
        "def main():\n",
        "    x_instances = [[2.7810836, 2.550537003],[1.465489372, 2.362125076],[3.396561688, 4.400293529], [1.38807019, 1.850220317],[3.06407232, 3.005305973],[7.627531214, 2.759262235],[5.332441248, 2.088626775],[6.922596716, 1.77106367],[8.675418651, -0.242068655],[7.673756466, 3.508563011]]\n",
        "    yi_label_instances = [0,0,0,0,0,1,1,1,1,1]\n",
        "\n",
        "    # Inicialização dos pesos e bias\n",
        "    w = [0, 0]\n",
        "    bias = 0\n",
        "\n",
        "    # Parâmetros\n",
        "    alpha = 0.1\n",
        "    num_epochs = 1000  # Definindo um número alto de épocas para garantir convergência\n",
        "\n",
        "    # Treinamento do Perceptron\n",
        "    for epoch in range(num_epochs):\n",
        "        total_gradiente_bias = 0\n",
        "        total_gradiente_x = [0, 0]\n",
        "        for i, x in enumerate(x_instances):\n",
        "            yi = yi_label_instances[i]\n",
        "            y_hat = predicao(w, x, bias)\n",
        "            gradiente_bias = (yi - y_hat) * (sigmoid(np.dot(w, x) + bias) * (1 - sigmoid(np.dot(w, x) + bias)))\n",
        "            total_gradiente_bias += gradiente_bias\n",
        "            for j in range(len(w)):\n",
        "                gradiente_x = (yi - y_hat) * (sigmoid(np.dot(w, x) + bias) * (1 - sigmoid(np.dot(w, x) + bias))) * x[j]\n",
        "                total_gradiente_x[j] += gradiente_x\n",
        "\n",
        "        bias = bias - alpha * total_gradiente_bias\n",
        "        for j in range(len(w)):\n",
        "            w[j] = w[j] - alpha * total_gradiente_x[j]\n",
        "\n",
        "    print(\"Peso=\", w)\n",
        "    print(\"Bias:\", bias)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbl88ZtHE5lg",
        "outputId": "4e05babe-f031-4180-b5c3-1faf4aca15a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peso= [-3.4555763189900004, 5.078439948669257]\n",
            "Bias: 1.9864656362642006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para obter os gradientes dos pesos e do bias em um algoritmo de treinamento de rede neural, como o Perceptron, é necessário utilizar o método do gradiente descendente, que é uma técnica de otimização amplamente utilizada em aprendizado de máquina (ML).\n",
        "\n",
        "O método do gradiente descendente consiste em calcular o gradiente da função de perda em relação aos parâmetros do modelo (pesos e bias) e, em seguida, ajustar esses parâmetros na direção oposta ao gradiente, com uma certa taxa de aprendizado, para minimizar a função de perda.\n",
        "\n",
        "Para calcular os gradientes, é necessário primeiramente definir uma função de perda, que mede o quão distante as previsões do modelo estão dos valores verdadeiros. Neste caso, podemos usar a função de perda de entropia cruzada, que é comumente utilizada em problemas de classificação binária e é compatível com a função de ativação sigmoidal.\n",
        "\n",
        "Uma vez que temos a função de perda definida, podemos calcular o gradiente da função de perda em relação a cada peso e ao bias usando o método de diferenciação. Este processo envolve calcular as derivadas parciais da função de perda em relação a cada parâmetro do modelo. Isso pode ser feito utilizando regras de derivação básicas e regras da cadeia.\n",
        "\n",
        "Em resumo, para obter os gradientes dos pesos e do bias em um algoritmo de treinamento de rede neural, precisamos:\n",
        "\n",
        "Definir uma função de perda adequada.\n",
        "Calcular as derivadas parciais da função de perda em relação a cada peso e ao bias.\n",
        "Usar essas derivadas parciais para ajustar os pesos e o bias na direção oposta ao gradiente, com uma certa taxa de aprendizado.\n",
        "Este processo é iterado várias vezes até que os pesos e o bias se convertem para valores que minimizam a função de perda e, portanto, otimizam o desempenho do modelo."
      ],
      "metadata": {
        "id": "FZ2eIJMIHY_7"
      }
    }
  ]
}